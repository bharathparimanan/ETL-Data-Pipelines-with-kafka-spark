# Kafka-Spark ETL Pipeline in Jupyter Notebooks

A simple end-to-end ETL pipeline using **Apache Kafka** and **Apache Spark**, built entirely in **Jupyter Notebooks**.

## ğŸ” Overview

This project demonstrates how to:
- Extract data from an API and stream it to Kafka
- Ingest the Kafka stream into Apache Spark
- Apply transformations and save the output as Parquet files

## ğŸ“ Project Structure

kafka-spark-etl/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ pipeline.ipynb                # Extract: API to Kafka & Ingest + Transform: Kafka to Spark & Save: Spark output to Parquet
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ README.md                         # Project overview
â””â”€â”€ .gitignore                        # Files to exclude from Git


## ğŸ”§ Technologies Used

- **Python**
- **Apache Kafka**
- **Apache Spark (Structured Streaming)**
- **Jupyter Notebooks**
- **Parquet (as output format)**

## ğŸš€ How to Run

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/kafka-spark-etl.git
cd kafka-spark-etl

pip install -r requirements.txt
